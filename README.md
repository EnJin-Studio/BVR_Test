# ğŸ¬ Bilibili Video View Count Prediction

This project aims to predict the view count of Bilibili homepage-recommended videos using multimodal input features, including thumbnail images, video titles, uploader information, and structured metadata. It integrates CLIP-based feature extraction, structured preprocessing, and regression model training for view prediction and video content scoring.

---

## ğŸš€ Project Objectives

- Crawl homepage video data from Bilibili (thumbnails, titles, uploader info, etc.)
- Extract semantic feature vectors using OpenAIâ€™s CLIP model
- Integrate additional structured metadata (uploader followers, video duration, time since publication)
- Construct a unified 1039-dimensional input feature vector for each video
- Train a regression model to predict view counts
- Provide a scalable framework for content analysis, scoring, and recommendation

---

## ğŸ“ Project Structure

```

bilibili-view-predictor/
â”œâ”€â”€ data/                        # Feature storage
â”‚   â”œâ”€â”€ clip\_image\_features.npy
â”‚   â”œâ”€â”€ clip\_text\_features.npy
â”‚   â”œâ”€â”€ uploader\_name\_features.npy
â”‚   â””â”€â”€ video\_meta.csv
â”œâ”€â”€ images/                      # Downloaded thumbnails
â”œâ”€â”€ scripts/                     # Core functionality
â”‚   â”œâ”€â”€ bilibili_web_crawler.py  # Scrape homepage video & uploader info
â”‚   â”œâ”€â”€ extract\_features.py     # Extract CLIP-based image & text features
â”‚   â”œâ”€â”€ build\_dataset.py        # Combine all features into training data
â”‚   â””â”€â”€ train\_model.py          # Train MLP regression model
â”œâ”€â”€ model/                       # Trained models
â”‚   â””â”€â”€ mlp\_regressor.pth
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ”¢ Input Features (1039 Dimensions)

| Modality    | Feature                    | Dim | Description                                        |
|-------------|----------------------------|-----|----------------------------------------------------|
| Image       | Thumbnail embedding (CLIP) | 512 | Extracted using `CLIP.encode_image()`              |
| Text        | Title embedding (CLIP)     | 512 | Extracted using `CLIP.encode_text()`               |
| Text        | Uploader name embedding    | 12  | Encoded using CLIP (short-text semantic vector)    |
| Numeric     | Uploader follower count    | 1   | Log-transformed with `log1p(follower)`             |
| Numeric     | Video duration (in seconds)| 1   | Original video length                              |
| Numeric     | Time since published (days)| 1   | Current time minus upload timestamp                |

ğŸ“ **Total: 512 + 512 + 12 + 3 = 1039 features**

---

## ğŸ¯ Prediction Target

- `view_count` â€” the number of video views  
- Transformed using `log1p(view_count)` to normalize extreme values

---

## ğŸ’» Installation

```bash
pip install -r requirements.txt
````

Key dependencies include:

* `torch`, `clip-by-openai`, `pandas`, `numpy`, `requests`, `tqdm`

---

## ğŸ› ï¸ Usage Instructions

### 1. Fetch homepage video data

```bash
python scripts/fetch_data.py
```

### 2. Extract CLIP-based features (image, title, uploader name)

```bash
python scripts/extract_features.py
```

### 3. Build the full dataset (1039D input, log view count as target)

```bash
python scripts/build_dataset.py
```

### 4. Train the regression model

```bash
python scripts/train_model.py
```

---

## ğŸ“Š Example Use Case

Once trained, the model can be used to:

* Estimate the expected view count of any new video
* Provide content creators with scoring feedback (image/text performance)
* Support a recommendation pipeline for homepage candidate ranking
* Identify high-potential videos before publishing

---

## ğŸ§ª Planned Features (Backlog)

* Add more metadata (e.g., likes, favorites, video category tags)
* Visualize high-dimensional feature space via PCA/t-SNE
* Explore image-text attention fusion with cross-modal transformers
* Build a Streamlit-based demo for real-time content evaluation
* Extend model to multitask prediction (views + likes + comments)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ‘¤ Authors

**Ziyuan Chu (Eric)**  
Boston University Â· Computer Engineering  
Email: czyuan@bu.edu

**Feng Tai (Jimmy)**  
Boston University Â· Computer Engineering  
Email: jimmytai@bu.edu



