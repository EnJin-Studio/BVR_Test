import time
import csv
import requests
import random
import os
import pandas as pd
from datetime import datetime
from html import unescape
import re
import math

# Clean title
def clean_html(text):
    text = unescape(text)
    return re.sub(r'<[^>]+>', '', text)

def parse_duration_to_seconds(duration_str):
    parts = duration_str.split(":")
    try:
        if len(parts) == 3:
            h, m, s = map(int, parts)
            return h * 3600 + m * 60 + s
        elif len(parts) == 2:
            m, s = map(int, parts)
            return m * 60 + s
        else:
            return 0
    except:
        return 0

# Simulate browser request to Bilibili homepage, fetch data from Bilibili API
# User-Agent represents the browser making the request
# Referer indicates that the request is from Bilibili
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
    "Referer": "https://search.bilibili.com",
    "Origin": "https://search.bilibili.com",
    "Accept": "application/json",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cookie": "_uuid=810D1082B9-F1073-81106-59101-E759C98F3F6F91420infoc; buvid_fp=4efefeec27f4b6797c2d67e3f6431784; buvid3=5DCD4719-61DA-AB7F-DAB1-8E4264DDC26A05662infoc; b_nut=1749940793; buvid4=445080EE-6521-E55B-622B-BD87E123894305662-025061506-Csu38aAzj1bTMozpd574CA%3D%3D; header_theme_version=CLOSE; enable_web_push=DISABLE; enable_feed_channel=ENABLE; rpdid=|(ukR|ukYl))0J'u~RmmR)J|l; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTAyNTkwMzMsImlhdCI6MTc0OTk5OTc3MywicGx0IjotMX0.M1su7Q6pBFjh5C2E3GJmWup2shxdFnr6nLfE8qs2lbA; bili_ticket_expires=1750258973; CURRENT_FNVAL=4048; b_lsid=558EB1068_1977A7FD606; home_feed_column=5; browser_resolution=1693-866"
}

# Partition List
PARTITIONS = {
    1: "åŠ¨ç”»",
    3: "éŸ³ä¹",
    4: "æ¸¸æˆ",
    5: "å¨±ä¹",
    11: "ç•ªå‰§",
    13: "çºªå½•ç‰‡",
    23: "ç”µè§†å‰§",
    36: "çŸ¥è¯†",
    83: "ç”µå½±",
    119: "å›½åˆ›",
    129: "åŠ¨ç‰©åœˆ",
    155: "åŠ¨ç”»ç»¼åˆ",
    160: "ç”Ÿæ´»",
    167: "èˆè¹ˆ",
    177: "é¬¼ç•œ",
    181: "çŸ¥è¯†ç»¼åˆ",
    188: "è¿åŠ¨",
    211: "æ—¶å°š",
    217: "ç§‘æŠ€",
    223: "ç¾é£Ÿ",
    234: "æ±½è½¦"
}


# Get follower count for uploader (Bilibili separates video data from user data)
# mid is the uploaderâ€™s user ID; follower info is under data
def get_follower_count(mid):
    url = "https://api.bilibili.com/x/relation/stat"
    params = {"vmid": mid}
    try:
        resp = requests.get(url, params=params, headers=HEADERS, timeout=5)
        resp.raise_for_status()
        data = resp.json()
        return data.get("data", {}).get("follower", None)
    except Exception as e:
        print(f"[ERROR] è·å–mid={mid}çš„ç²‰ä¸æ•°å¤±è´¥: {e}")
        return None

# Extract required fields
def parse_partition_items(vlist):
    items = []
    for v in vlist:
        aid = v.get("aid")
        title = v.get("title")
        pic = v.get("pic")
        duration = v.get("duration", 0)
        pubdate_ts = v.get("pubdate")
        view_count = v.get("stat", {}).get("view")
        danmaku_count = v.get("stat", {}).get("danmaku")
        uploader_name = v.get("owner", {}).get("name")
        mid = v.get("owner", {}).get("mid")

        items.append({
            "aid": aid,
            "title": clean_html(title),
            "pic": pic,
            "duration": duration,
            "pub_seconds_ago": int(time.time() - pubdate_ts) if pubdate_ts else None,
            "view_count": view_count,
            "danmaku_count": danmaku_count,
            "uploader_name": uploader_name,
            "uploader_mid": mid
        })
    print(f"[DEBUG] parse_partition_items æå– {len(items)} æ¡è§†é¢‘")
    return items

def crawl_partition(rid, name, delay=1):
    print(f"\nğŸ“¥ æ­£åœ¨çˆ¬å–åˆ†åŒº [{name}]")
    seen = set()
    all_items = []
    max_page = 100  # é»˜è®¤æœ€å¤š100é¡µ

    for pn in range(1, max_page + 1):
        print(f" â†’ ç¬¬ {pn} é¡µ")
        url = f"https://api.bilibili.com/x/web-interface/dynamic/region?rid={rid}&pn={pn}&ps=50"
        try:
            resp = requests.get(url, headers=HEADERS, timeout=10)
            print(f"[DEBUG] è¯·æ±‚ URL: {url}")
            print(f"[DEBUG] å“åº”çŠ¶æ€ç : {resp.status_code}")
            data = resp.json()
        except Exception as e:
            print(f"[ERROR] è¯·æ±‚æˆ–è§£æ JSON å¤±è´¥: {e}")
            break

        data_obj = data.get("data", None)
        if not data_obj or "archives" not in data_obj:
            print("[DEBUG] æœ¬é¡µæ— æ•°æ®æˆ–æ ¼å¼å¼‚å¸¸ï¼Œè·³å‡ºå¾ªç¯")
            break

        if pn == 1:
            count = data_obj.get("page", {}).get("count", 0)
            size = data_obj.get("page", {}).get("size", 50)
            if count and size:
                max_page = math.ceil(count / size)
                print(f"[DEBUG] æ€»è§†é¢‘æ•°: {count}ï¼Œæ¯é¡µ: {size}ï¼Œæ€»é¡µæ•°: {max_page}")

        vlist = data_obj.get("archives", [])
        print(f"[DEBUG] æœ¬é¡µè§†é¢‘æ•°é‡: {len(vlist)}")
        if vlist:
            print(f"[DEBUG] ç¤ºä¾‹è§†é¢‘æ ‡é¢˜: {vlist[0].get('title')}")

        new_items = parse_partition_items(vlist)
        print(f"[DEBUG] parse_partition_items æå– {len(new_items)} æ¡è§†é¢‘")

        for item in new_items:
            aid = item["aid"]
            if aid and aid not in seen:
                item["uploader_follower"] = None
                del item["uploader_mid"]
                all_items.append(item)
                seen.add(aid)

        print(f" â†’ å½“å‰æ€»æ•°: {len(all_items)}")
        time.sleep(random.uniform(delay, 2))

    return all_items


def main():
    os.makedirs("csv_data", exist_ok=True)
    all_data = []

    for rid, name in PARTITIONS.items():
        part_items = crawl_partition(rid, name)
        if part_items:
            df = pd.DataFrame(part_items)
            df.to_csv(f"csv_data/{name}_{rid}.csv", index=False, encoding="utf-8-sig")
            all_data.extend(part_items)
            print(f"âœ… åˆ†åŒº [{name}] å®Œæˆï¼Œå·²ä¿å­˜ {len(part_items)} æ¡")

    df_all = pd.DataFrame(all_data)
    df_all.drop_duplicates(subset="aid", inplace=True)
    df_all.to_csv("csv_data/all_partitions_combined.csv", index=False, encoding="utf-8-sig")
    print(f"\nğŸ‰ æ‰€æœ‰åˆ†åŒºåˆå¹¶å®Œæˆï¼Œå…± {len(df_all)} æ¡")

if __name__ == "__main__":
    main()
